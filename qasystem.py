# -*- coding: utf-8 -*-
"""Copy of qasystem.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d1gHltPGCtkylovWhC1sG5Ov3sT6CegY
"""

pip install tensorflow

pip install transformers

from transformers import BertForQuestionAnswering

modelname = 'deepset/bert-base-cased-squad2'

model = BertForQuestionAnswering.from_pretrained(modelname)

from transformers import AutoTokenizer

tokenizer=AutoTokenizer.from_pretrained('deepset/bert-base-cased-squad2')

context=(
    "The Intergovernmental Panel on Climate Change (IPCC) is a scientific intergovernmental body under the auspices " 
    "of the United Nations, set up at the request of member governments. It was first established in 1988 by two United" 
    "Nations organizations, the World Meteorological Organization (WMO) and the United Nations Environment Programme "
    "(UNEP), and later endorsed by the United Nations General Assembly through Resolution 43/53. Membership of the IPCC " 
    "is open to all members of the WMO and UNEP. The IPCC produces reports that support the United Nations Framework" 
    "Convention Climate Change (UNFCCC), which is the main international treaty on climate change. The ultimate on "
    "objective of the UNFCCC is to \"stabilize greenhouse gas concentrations in the atmosphere at a level that would " 
    "prevent dangerous anthropogenic [1.e., human-induced] interference with the climate system\". IPCC reports cover " 
    "\"the scientific, technical and socio-economic information relevant to understanding the scientific basis of risk "
    "of human-induced climate change, its potential impacts and options for adaptation and mitigation.\""
)

context

questions = [

"What organization is the IPCC a part of?", 
"What UN organizations established the IPCC?", 
"What does the UN want to stabilize?"
]

tokenizer.encode(questions[0],truncation=True,padding=True)

from transformers import pipeline

nlp=pipeline('question-answering',model=model,tokenizer=tokenizer)

nlp({
    'question':questions[0],
     'context':context
})

context[118:132]

nlp({
    'question':questions[1],
     'context':context
})

context[287:343]

nlp({
    'question':questions[2],
     'context':context
})

context[714:761]

intro=(
    "my name is kushagra trivedi,i was born in 6th May 2001,"
    " i love to play football and my favourite hobby is reading non-fiction novels"
)
intro

ques = [
"when was kushagra born?",
"what he loves to play?",
"what is his favourite hobby?"
]

tokenizer.encode(ques[0],truncation=True,padding=True)

from transformers import pipeline

ans=pipeline('question-answering',model=model,tokenizer=tokenizer)

ans({
    'question':ques[0],
     'context':intro
    
})

intro[42:54]

ans({
    'question':ques[1],
     'context':intro
    
})

intro[71:79]

ans({
    'question':ques[2],
     'context':intro
    
})

intro[106:132]

pip install wikipedia

import pprint as pp

question='what is the wingspan of an albatross?'

import wikipedia as wiki

results = wiki.search(question)

print("wikipedia search results for our question:\n" )
pp.pprint(results)

page=wiki.page(results[0])
text=page.content
print(f"\n The {results[0]} Wikipedia article contains {len(text)} characters.")

question='the father of the nation?'

results = wiki.search(question)

pp.pprint(results)
page=wiki.page(results[0])

inputs = tokenizer.encode_plus(question, text, return_tensors='pt')
print(f"This translates into {len(inputs['input_ids'][0])} tokens.")

import torch

# time to chunk!
from collections import OrderedDict

# identify question tokens (token_type_ids = 0)
qmask = inputs['token_type_ids'].lt(1)
qt = torch.masked_select(inputs['input_ids'], qmask)
print(f"The question consists of {qt.size()[0]} tokens.")

chunk_size = model.config.max_position_embeddings - qt.size()[0] - 1 # the "-1" accounts for
# having to add a [SEP] token to the end of each chunk
print(f"Each chunk will contain {chunk_size - 2} tokens of the Wikipedia article.")

# create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input
chunked_input = OrderedDict()
for k,v in inputs.items():
    q = torch.masked_select(v, qmask)
    c = torch.masked_select(v, ~qmask)
    chunks = torch.split(c, chunk_size)

    for i, chunk in enumerate(chunks):
        if i not in chunked_input:
            chunked_input[i] = {}

        thing = torch.cat((q, chunk))
        if i != len(chunks)-1:
            if k == 'input_ids':
                thing = torch.cat((thing, torch.tensor([102])))
            else:
                thing = torch.cat((thing, torch.tensor([1])))

        chunked_input[i][k] = torch.unsqueeze(thing, dim=0)

for i in range(len(chunked_input.keys())):
    print(f"Number of tokens in chunk {i}: {len(chunked_input[i]['input_ids'].tolist()[0])}")

pip install transformers==3

def convert_ids_to_string(tokenizer, input_ids):
    return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids))

answer = ''

# now we iterate over our chunks, looking for the best answer from each chunk
for _, chunk in chunked_input.items():
    answer_start_scores, answer_end_scores = model(**chunk)

    answer_start = torch.argmax(answer_start_scores)
    answer_end = torch.argmax(answer_end_scores) + 1

    ans = convert_ids_to_string(tokenizer, chunk['input_ids'][0][answer_start:answer_end])
    
    # if the ans == [CLS] then the model did not find a real answer in this chunk
    if ans != '[CLS]':
        answer += ans + " / "
        
print(answer)

from transformers import AutoTokenizer, AutoModelForQuestionAnswering


class DocumentReader:
    def __init__(self, pretrained_model_name_or_path='bert-large-uncased'):
        self.READER_PATH = pretrained_model_name_or_path
        self.tokenizer = AutoTokenizer.from_pretrained(self.READER_PATH)
        self.model = AutoModelForQuestionAnswering.from_pretrained(self.READER_PATH)
        self.max_len = self.model.config.max_position_embeddings
        self.chunked = False

    def tokenize(self, question, text):
        self.inputs = self.tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors="pt")
        self.input_ids = self.inputs["input_ids"].tolist()[0]

        if len(self.input_ids) > self.max_len:
            self.inputs = self.chunkify()
            self.chunked = True

    def chunkify(self):
        """ 
        Break up a long article into chunks that fit within the max token
        requirement for that Transformer model. 

        Calls to BERT / RoBERTa / ALBERT require the following format:
        [CLS] question tokens [SEP] context tokens [SEP].
        """

        # create question mask based on token_type_ids
        # value is 0 for question tokens, 1 for context tokens
        qmask = self.inputs['token_type_ids'].lt(1)
        qt = torch.masked_select(self.inputs['input_ids'], qmask)
        chunk_size = self.max_len - qt.size()[0] - 1 # the "-1" accounts for
        # having to add an ending [SEP] token to the end

        # create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input
        chunked_input = OrderedDict()
        for k,v in self.inputs.items():
            q = torch.masked_select(v, qmask)
            c = torch.masked_select(v, ~qmask)
            chunks = torch.split(c, chunk_size)
            
            for i, chunk in enumerate(chunks):
                if i not in chunked_input:
                    chunked_input[i] = {}

                thing = torch.cat((q, chunk))
                if i != len(chunks)-1:
                    if k == 'input_ids':
                        thing = torch.cat((thing, torch.tensor([102])))
                    else:
                        thing = torch.cat((thing, torch.tensor([1])))

                chunked_input[i][k] = torch.unsqueeze(thing, dim=0)
        return chunked_input

    def get_answer(self):
        if self.chunked:
            answer = ''
            for k, chunk in self.inputs.items():
                answer_start_scores, answer_end_scores = self.model(**chunk)

                answer_start = torch.argmax(answer_start_scores)
                answer_end = torch.argmax(answer_end_scores) + 1

                ans = self.convert_ids_to_string(chunk['input_ids'][0][answer_start:answer_end])
                if ans != '[CLS]':
                    answer += ans + " / "
            return answer
        else:
            answer_start_scores, answer_end_scores = self.model(**self.inputs)

            answer_start = torch.argmax(answer_start_scores)  # get the most likely beginning of answer with the argmax of the score
            answer_end = torch.argmax(answer_end_scores) + 1  # get the most likely end of answer with the argmax of the score
        
            return self.convert_ids_to_string(self.inputs['input_ids'][0][
                                              answer_start:answer_end])

    def convert_ids_to_string(self, input_ids):
        return self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(input_ids))

questions = [
    'When was Barack Obama born?',
    'Why is the sky blue?',
    'How many sides does a pentagon have?'
]

reader = DocumentReader("deepset/bert-base-cased-squad2") 

# if you trained your own model using the training cell earlier, you can access it with this:
#reader = DocumentReader("./models/bert/bbu_squad2")

for question in questions:
    print(f"Question: {question}")
    results = wiki.search(question)
    page = wiki.page(results[1])
    print(f"Top wiki result: {page}")
    text = page.content
    reader.tokenize(question, text)
    print(f"Answer: {reader.get_answer()}")
    print()
